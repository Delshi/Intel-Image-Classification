digraph {
	graph [size="103.64999999999999,103.64999999999999"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1481012476288 [label="
 (2, 6)" fillcolor=darkolivegreen1]
	1480897847856 [label=AddmmBackward0]
	1480898175680 -> 1480897847856
	1481012470128 [label="classifier.5.bias
 (6)" fillcolor=lightblue]
	1481012470128 -> 1480898175680
	1480898175680 [label=AccumulateGrad]
	1480898180768 -> 1480897847856
	1480898180768 [label=NativeDropoutBackward0]
	1480899438064 -> 1480898180768
	1480899438064 [label=ReluBackward0]
	1480899439264 -> 1480899438064
	1480899439264 [label=AddmmBackward0]
	1480895945728 -> 1480899439264
	1481012470048 [label="classifier.2.bias
 (2048)" fillcolor=lightblue]
	1481012470048 -> 1480895945728
	1480895945728 [label=AccumulateGrad]
	1480895945680 -> 1480899439264
	1480895945680 [label=NativeDropoutBackward0]
	1480896913968 -> 1480895945680
	1480896913968 [label=ViewBackward0]
	1480487146528 -> 1480896913968
	1480487146528 [label=MeanBackward1]
	1480890587488 -> 1480487146528
	1480890587488 [label=NativeDropoutBackward0]
	1480890593152 -> 1480890587488
	1480890593152 [label=ReluBackward0]
	1480897588496 -> 1480890593152
	1480897588496 [label=AddBackward0]
	1480897587680 -> 1480897588496
	1480897587680 [label=CudnnBatchNormBackward0]
	1480897587776 -> 1480897587680
	1480897587776 [label=ConvolutionBackward0]
	1480899763296 -> 1480897587776
	1480899763296 [label=ReluBackward0]
	1480899769296 -> 1480899763296
	1480899769296 [label=CudnnBatchNormBackward0]
	1480899768192 -> 1480899769296
	1480899768192 [label=ConvolutionBackward0]
	1480899762912 -> 1480899768192
	1480899762912 [label=ReluBackward0]
	1480899763632 -> 1480899762912
	1480899763632 [label=CudnnBatchNormBackward0]
	1480899764064 -> 1480899763632
	1480899764064 [label=ConvolutionBackward0]
	1480897589696 -> 1480899764064
	1480897589696 [label=ReluBackward0]
	1480899764352 -> 1480897589696
	1480899764352 [label=AddBackward0]
	1480899764448 -> 1480899764352
	1480899764448 [label=CudnnBatchNormBackward0]
	1480899764592 -> 1480899764448
	1480899764592 [label=ConvolutionBackward0]
	1480899764784 -> 1480899764592
	1480899764784 [label=ReluBackward0]
	1480899764928 -> 1480899764784
	1480899764928 [label=CudnnBatchNormBackward0]
	1480899765024 -> 1480899764928
	1480899765024 [label=ConvolutionBackward0]
	1480899765216 -> 1480899765024
	1480899765216 [label=ReluBackward0]
	1480899765360 -> 1480899765216
	1480899765360 [label=CudnnBatchNormBackward0]
	1480899765456 -> 1480899765360
	1480899765456 [label=ConvolutionBackward0]
	1480899765648 -> 1480899765456
	1480899765648 [label=ReluBackward0]
	1480899765792 -> 1480899765648
	1480899765792 [label=AddBackward0]
	1480899765888 -> 1480899765792
	1480899765888 [label=CudnnBatchNormBackward0]
	1480899766032 -> 1480899765888
	1480899766032 [label=ConvolutionBackward0]
	1480899766224 -> 1480899766032
	1480899766224 [label=ReluBackward0]
	1480899766368 -> 1480899766224
	1480899766368 [label=CudnnBatchNormBackward0]
	1480899766464 -> 1480899766368
	1480899766464 [label=ConvolutionBackward0]
	1480899766656 -> 1480899766464
	1480899766656 [label=ReluBackward0]
	1480899766800 -> 1480899766656
	1480899766800 [label=CudnnBatchNormBackward0]
	1480899766896 -> 1480899766800
	1480899766896 [label=ConvolutionBackward0]
	1480899765936 -> 1480899766896
	1480899765936 [label=ReluBackward0]
	1480899767184 -> 1480899765936
	1480899767184 [label=AddBackward0]
	1480899767328 -> 1480899767184
	1480899767328 [label=CudnnBatchNormBackward0]
	1480899767616 -> 1480899767328
	1480899767616 [label=ConvolutionBackward0]
	1480899767712 -> 1480899767616
	1480899767712 [label=ReluBackward0]
	1480899767856 -> 1480899767712
	1480899767856 [label=CudnnBatchNormBackward0]
	1480899768048 -> 1480899767856
	1480899768048 [label=ConvolutionBackward0]
	1480899768336 -> 1480899768048
	1480899768336 [label=ReluBackward0]
	1480899768432 -> 1480899768336
	1480899768432 [label=CudnnBatchNormBackward0]
	1480899768624 -> 1480899768432
	1480899768624 [label=ConvolutionBackward0]
	1480899768816 -> 1480899768624
	1480899768816 [label=ReluBackward0]
	1480899768960 -> 1480899768816
	1480899768960 [label=AddBackward0]
	1480899769104 -> 1480899768960
	1480899769104 [label=CudnnBatchNormBackward0]
	1480899769392 -> 1480899769104
	1480899769392 [label=ConvolutionBackward0]
	1480899769584 -> 1480899769392
	1480899769584 [label=ReluBackward0]
	1480899769728 -> 1480899769584
	1480899769728 [label=CudnnBatchNormBackward0]
	1480899769872 -> 1480899769728
	1480899769872 [label=ConvolutionBackward0]
	1480899770064 -> 1480899769872
	1480899770064 [label=ReluBackward0]
	1480899770208 -> 1480899770064
	1480899770208 [label=CudnnBatchNormBackward0]
	1480899770352 -> 1480899770208
	1480899770352 [label=ConvolutionBackward0]
	1480899768912 -> 1480899770352
	1480899768912 [label=ReluBackward0]
	1480899770640 -> 1480899768912
	1480899770640 [label=AddBackward0]
	1480899770784 -> 1480899770640
	1480899770784 [label=CudnnBatchNormBackward0]
	1480899770928 -> 1480899770784
	1480899770928 [label=ConvolutionBackward0]
	1480899771120 -> 1480899770928
	1480899771120 [label=ReluBackward0]
	1480899771264 -> 1480899771120
	1480899771264 [label=CudnnBatchNormBackward0]
	1480899771408 -> 1480899771264
	1480899771408 [label=ConvolutionBackward0]
	1480899771600 -> 1480899771408
	1480899771600 [label=ReluBackward0]
	1480899771744 -> 1480899771600
	1480899771744 [label=CudnnBatchNormBackward0]
	1480899771888 -> 1480899771744
	1480899771888 [label=ConvolutionBackward0]
	1480899772080 -> 1480899771888
	1480899772080 [label=ReluBackward0]
	1480899772224 -> 1480899772080
	1480899772224 [label=AddBackward0]
	1480899772368 -> 1480899772224
	1480899772368 [label=CudnnBatchNormBackward0]
	1481012641952 -> 1480899772368
	1481012641952 [label=ConvolutionBackward0]
	1481012642144 -> 1481012641952
	1481012642144 [label=ReluBackward0]
	1481012642288 -> 1481012642144
	1481012642288 [label=CudnnBatchNormBackward0]
	1481012642432 -> 1481012642288
	1481012642432 [label=ConvolutionBackward0]
	1481012642624 -> 1481012642432
	1481012642624 [label=ReluBackward0]
	1481012642768 -> 1481012642624
	1481012642768 [label=CudnnBatchNormBackward0]
	1481012642912 -> 1481012642768
	1481012642912 [label=ConvolutionBackward0]
	1480899772176 -> 1481012642912
	1480899772176 [label=ReluBackward0]
	1481012643200 -> 1480899772176
	1481012643200 [label=AddBackward0]
	1481012643344 -> 1481012643200
	1481012643344 [label=CudnnBatchNormBackward0]
	1481012643488 -> 1481012643344
	1481012643488 [label=ConvolutionBackward0]
	1481012643680 -> 1481012643488
	1481012643680 [label=ReluBackward0]
	1481012643824 -> 1481012643680
	1481012643824 [label=CudnnBatchNormBackward0]
	1481012643968 -> 1481012643824
	1481012643968 [label=ConvolutionBackward0]
	1481012644160 -> 1481012643968
	1481012644160 [label=ReluBackward0]
	1481012644304 -> 1481012644160
	1481012644304 [label=CudnnBatchNormBackward0]
	1481012644448 -> 1481012644304
	1481012644448 [label=ConvolutionBackward0]
	1481012644640 -> 1481012644448
	1481012644640 [label=ReluBackward0]
	1481012644784 -> 1481012644640
	1481012644784 [label=AddBackward0]
	1481012644928 -> 1481012644784
	1481012644928 [label=CudnnBatchNormBackward0]
	1481012645072 -> 1481012644928
	1481012645072 [label=ConvolutionBackward0]
	1481012645264 -> 1481012645072
	1481012645264 [label=ReluBackward0]
	1481012645408 -> 1481012645264
	1481012645408 [label=CudnnBatchNormBackward0]
	1481012645552 -> 1481012645408
	1481012645552 [label=ConvolutionBackward0]
	1481012645744 -> 1481012645552
	1481012645744 [label=ReluBackward0]
	1481012645888 -> 1481012645744
	1481012645888 [label=CudnnBatchNormBackward0]
	1481012646032 -> 1481012645888
	1481012646032 [label=ConvolutionBackward0]
	1481012644736 -> 1481012646032
	1481012644736 [label=ReluBackward0]
	1481012646320 -> 1481012644736
	1481012646320 [label=AddBackward0]
	1481012646464 -> 1481012646320
	1481012646464 [label=CudnnBatchNormBackward0]
	1481012646608 -> 1481012646464
	1481012646608 [label=ConvolutionBackward0]
	1481012646800 -> 1481012646608
	1481012646800 [label=ReluBackward0]
	1481012646944 -> 1481012646800
	1481012646944 [label=CudnnBatchNormBackward0]
	1481012647088 -> 1481012646944
	1481012647088 [label=ConvolutionBackward0]
	1481012647280 -> 1481012647088
	1481012647280 [label=ReluBackward0]
	1481012647424 -> 1481012647280
	1481012647424 [label=CudnnBatchNormBackward0]
	1481012647568 -> 1481012647424
	1481012647568 [label=ConvolutionBackward0]
	1481012646176 -> 1481012647568
	1481012646176 [label=MaxPool2DWithIndicesBackward0]
	1481012647856 -> 1481012646176
	1481012647856 [label=ReluBackward0]
	1481012648000 -> 1481012647856
	1481012648000 [label=CudnnBatchNormBackward0]
	1481012648144 -> 1481012648000
	1481012648144 [label=ConvolutionBackward0]
	1481012648336 -> 1481012648144
	1480308321296 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	1480308321296 -> 1481012648336
	1481012648336 [label=AccumulateGrad]
	1481012648048 -> 1481012648000
	1480308321216 [label="bn1.weight
 (64)" fillcolor=lightblue]
	1480308321216 -> 1481012648048
	1481012648048 [label=AccumulateGrad]
	1481012648192 -> 1481012648000
	1480308314336 [label="bn1.bias
 (64)" fillcolor=lightblue]
	1480308314336 -> 1481012648192
	1481012648192 [label=AccumulateGrad]
	1481012647760 -> 1481012647568
	1480308320896 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1480308320896 -> 1481012647760
	1481012647760 [label=AccumulateGrad]
	1481012647376 -> 1481012647424
	1480308313856 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	1480308313856 -> 1481012647376
	1481012647376 [label=AccumulateGrad]
	1481012647616 -> 1481012647424
	1480308313776 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	1480308313776 -> 1481012647616
	1481012647616 [label=AccumulateGrad]
	1481012647328 -> 1481012647088
	1480308320416 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1480308320416 -> 1481012647328
	1481012647328 [label=AccumulateGrad]
	1481012646896 -> 1481012646944
	1480308313536 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	1480308313536 -> 1481012646896
	1481012646896 [label=AccumulateGrad]
	1481012647136 -> 1481012646944
	1480308313456 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	1480308313456 -> 1481012647136
	1481012647136 [label=AccumulateGrad]
	1481012646848 -> 1481012646608
	1480308313056 [label="layer1.0.conv3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1480308313056 -> 1481012646848
	1481012646848 [label=AccumulateGrad]
	1481012646656 -> 1481012646464
	1480308320176 [label="layer1.0.bn3.weight
 (64)" fillcolor=lightblue]
	1480308320176 -> 1481012646656
	1481012646656 [label=AccumulateGrad]
	1481012646560 -> 1481012646464
	1480308320096 [label="layer1.0.bn3.bias
 (64)" fillcolor=lightblue]
	1480308320096 -> 1481012646560
	1481012646560 [label=AccumulateGrad]
	1481012646176 -> 1481012646320
	1481012646224 -> 1481012646032
	1480308319856 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1480308319856 -> 1481012646224
	1481012646224 [label=AccumulateGrad]
	1481012645840 -> 1481012645888
	1480308314016 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	1480308314016 -> 1481012645840
	1481012645840 [label=AccumulateGrad]
	1481012646080 -> 1481012645888
	1480308313936 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	1480308313936 -> 1481012646080
	1481012646080 [label=AccumulateGrad]
	1481012645792 -> 1481012645552
	1480308325216 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1480308325216 -> 1481012645792
	1481012645792 [label=AccumulateGrad]
	1481012645360 -> 1481012645408
	1480308312176 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	1480308312176 -> 1481012645360
	1481012645360 [label=AccumulateGrad]
	1481012645600 -> 1481012645408
	1480308312096 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	1480308312096 -> 1481012645600
	1481012645600 [label=AccumulateGrad]
	1481012645312 -> 1481012645072
	1480899828288 [label="layer1.1.conv3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	1480899828288 -> 1481012645312
	1481012645312 [label=AccumulateGrad]
	1481012645120 -> 1481012644928
	1480899827728 [label="layer1.1.bn3.weight
 (64)" fillcolor=lightblue]
	1480899827728 -> 1481012645120
	1481012645120 [label=AccumulateGrad]
	1481012645024 -> 1481012644928
	1480899827808 [label="layer1.1.bn3.bias
 (64)" fillcolor=lightblue]
	1480899827808 -> 1481012645024
	1481012645024 [label=AccumulateGrad]
	1481012644736 -> 1481012644784
	1481012644688 -> 1481012644448
	1480899829088 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	1480899829088 -> 1481012644688
	1481012644688 [label=AccumulateGrad]
	1481012644256 -> 1481012644304
	1480899829168 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	1480899829168 -> 1481012644256
	1481012644256 [label=AccumulateGrad]
	1481012644496 -> 1481012644304
	1480899829248 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	1480899829248 -> 1481012644496
	1481012644496 [label=AccumulateGrad]
	1481012644208 -> 1481012643968
	1480899829728 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1480899829728 -> 1481012644208
	1481012644208 [label=AccumulateGrad]
	1481012643776 -> 1481012643824
	1480899829808 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	1480899829808 -> 1481012643776
	1481012643776 [label=AccumulateGrad]
	1481012644016 -> 1481012643824
	1480899829888 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	1480899829888 -> 1481012644016
	1481012644016 [label=AccumulateGrad]
	1481012643728 -> 1481012643488
	1480899830848 [label="layer2.0.conv3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1480899830848 -> 1481012643728
	1481012643728 [label=AccumulateGrad]
	1481012643536 -> 1481012643344
	1480899830288 [label="layer2.0.bn3.weight
 (128)" fillcolor=lightblue]
	1480899830288 -> 1481012643536
	1481012643536 [label=AccumulateGrad]
	1481012643440 -> 1481012643344
	1480899830368 [label="layer2.0.bn3.bias
 (128)" fillcolor=lightblue]
	1480899830368 -> 1481012643440
	1481012643440 [label=AccumulateGrad]
	1481012643056 -> 1481012643200
	1481012643056 [label=CudnnBatchNormBackward0]
	1481012643872 -> 1481012643056
	1481012643872 [label=ConvolutionBackward0]
	1481012644640 -> 1481012643872
	1481012644112 -> 1481012643872
	1480899828448 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	1480899828448 -> 1481012644112
	1481012644112 [label=AccumulateGrad]
	1481012643632 -> 1481012643056
	1480899828528 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	1480899828528 -> 1481012643632
	1481012643632 [label=AccumulateGrad]
	1481012643584 -> 1481012643056
	1480899828608 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	1480899828608 -> 1481012643584
	1481012643584 [label=AccumulateGrad]
	1481012643104 -> 1481012642912
	1480899831008 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1480899831008 -> 1481012643104
	1481012643104 [label=AccumulateGrad]
	1481012642720 -> 1481012642768
	1480899831088 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	1480899831088 -> 1481012642720
	1481012642720 [label=AccumulateGrad]
	1481012642960 -> 1481012642768
	1480899831168 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	1480899831168 -> 1481012642960
	1481012642960 [label=AccumulateGrad]
	1481012642672 -> 1481012642432
	1480899831648 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1480899831648 -> 1481012642672
	1481012642672 [label=AccumulateGrad]
	1481012642240 -> 1481012642288
	1480899831728 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	1480899831728 -> 1481012642240
	1481012642240 [label=AccumulateGrad]
	1481012642480 -> 1481012642288
	1480899831808 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	1480899831808 -> 1481012642480
	1481012642480 [label=AccumulateGrad]
	1481012642192 -> 1481012641952
	1480899832768 [label="layer2.1.conv3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	1480899832768 -> 1481012642192
	1481012642192 [label=AccumulateGrad]
	1481012642000 -> 1480899772368
	1480899832208 [label="layer2.1.bn3.weight
 (128)" fillcolor=lightblue]
	1480899832208 -> 1481012642000
	1481012642000 [label=AccumulateGrad]
	1481012641904 -> 1480899772368
	1480899832288 [label="layer2.1.bn3.bias
 (128)" fillcolor=lightblue]
	1480899832288 -> 1481012641904
	1481012641904 [label=AccumulateGrad]
	1480899772176 -> 1480899772224
	1480899772128 -> 1480899771888
	1480899833568 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	1480899833568 -> 1480899772128
	1480899772128 [label=AccumulateGrad]
	1480899771696 -> 1480899771744
	1480899833648 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	1480899833648 -> 1480899771696
	1480899771696 [label=AccumulateGrad]
	1480899771936 -> 1480899771744
	1480899833728 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	1480899833728 -> 1480899771936
	1480899771936 [label=AccumulateGrad]
	1480899771648 -> 1480899771408
	1480899834208 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1480899834208 -> 1480899771648
	1480899771648 [label=AccumulateGrad]
	1480899771216 -> 1480899771264
	1480899834288 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	1480899834288 -> 1480899771216
	1480899771216 [label=AccumulateGrad]
	1480899771456 -> 1480899771264
	1480899834368 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	1480899834368 -> 1480899771456
	1480899771456 [label=AccumulateGrad]
	1480899771168 -> 1480899770928
	1480899835328 [label="layer3.0.conv3.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1480899835328 -> 1480899771168
	1480899771168 [label=AccumulateGrad]
	1480899770976 -> 1480899770784
	1480899834768 [label="layer3.0.bn3.weight
 (256)" fillcolor=lightblue]
	1480899834768 -> 1480899770976
	1480899770976 [label=AccumulateGrad]
	1480899770880 -> 1480899770784
	1480899834848 [label="layer3.0.bn3.bias
 (256)" fillcolor=lightblue]
	1480899834848 -> 1480899770880
	1480899770880 [label=AccumulateGrad]
	1480899770496 -> 1480899770640
	1480899770496 [label=CudnnBatchNormBackward0]
	1480899771312 -> 1480899770496
	1480899771312 [label=ConvolutionBackward0]
	1480899772080 -> 1480899771312
	1480899771552 -> 1480899771312
	1480899832928 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	1480899832928 -> 1480899771552
	1480899771552 [label=AccumulateGrad]
	1480899771072 -> 1480899770496
	1480899833008 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	1480899833008 -> 1480899771072
	1480899771072 [label=AccumulateGrad]
	1480899771024 -> 1480899770496
	1480899833088 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	1480899833088 -> 1480899771024
	1480899771024 [label=AccumulateGrad]
	1480899770544 -> 1480899770352
	1480899835488 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1480899835488 -> 1480899770544
	1480899770544 [label=AccumulateGrad]
	1480899770160 -> 1480899770208
	1480899835568 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	1480899835568 -> 1480899770160
	1480899770160 [label=AccumulateGrad]
	1480899770400 -> 1480899770208
	1480899835648 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	1480899835648 -> 1480899770400
	1480899770400 [label=AccumulateGrad]
	1480899770112 -> 1480899769872
	1480899836128 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1480899836128 -> 1480899770112
	1480899770112 [label=AccumulateGrad]
	1480899769680 -> 1480899769728
	1480899836208 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	1480899836208 -> 1480899769680
	1480899769680 [label=AccumulateGrad]
	1480899769920 -> 1480899769728
	1480899836288 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	1480899836288 -> 1480899769920
	1480899769920 [label=AccumulateGrad]
	1480899769632 -> 1480899769392
	1480899837248 [label="layer3.1.conv3.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	1480899837248 -> 1480899769632
	1480899769632 [label=AccumulateGrad]
	1480899769440 -> 1480899769104
	1480899836688 [label="layer3.1.bn3.weight
 (256)" fillcolor=lightblue]
	1480899836688 -> 1480899769440
	1480899769440 [label=AccumulateGrad]
	1480899769344 -> 1480899769104
	1480899836768 [label="layer3.1.bn3.bias
 (256)" fillcolor=lightblue]
	1480899836768 -> 1480899769344
	1480899769344 [label=AccumulateGrad]
	1480899768912 -> 1480899768960
	1480899768864 -> 1480899768624
	1481012461728 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	1481012461728 -> 1480899768864
	1480899768864 [label=AccumulateGrad]
	1480899768384 -> 1480899768432
	1481012461808 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	1481012461808 -> 1480899768384
	1480899768384 [label=AccumulateGrad]
	1480899768672 -> 1480899768432
	1481012461888 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	1481012461888 -> 1480899768672
	1480899768672 [label=AccumulateGrad]
	1480899768240 -> 1480899768048
	1481012462368 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1481012462368 -> 1480899768240
	1480899768240 [label=AccumulateGrad]
	1480899767952 -> 1480899767856
	1481012462448 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	1481012462448 -> 1480899767952
	1480899767952 [label=AccumulateGrad]
	1480899768096 -> 1480899767856
	1481012462528 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	1481012462528 -> 1480899768096
	1480899768096 [label=AccumulateGrad]
	1480899767760 -> 1480899767616
	1481012463488 [label="layer4.0.conv3.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1481012463488 -> 1480899767760
	1480899767760 [label=AccumulateGrad]
	1480899767520 -> 1480899767328
	1481012462928 [label="layer4.0.bn3.weight
 (512)" fillcolor=lightblue]
	1481012462928 -> 1480899767520
	1480899767520 [label=AccumulateGrad]
	1480899767424 -> 1480899767328
	1481012463008 [label="layer4.0.bn3.bias
 (512)" fillcolor=lightblue]
	1481012463008 -> 1480899767424
	1480899767424 [label=AccumulateGrad]
	1480899767040 -> 1480899767184
	1480899767040 [label=CudnnBatchNormBackward0]
	1480899767904 -> 1480899767040
	1480899767904 [label=ConvolutionBackward0]
	1480899768816 -> 1480899767904
	1480899768288 -> 1480899767904
	1480899837408 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	1480899837408 -> 1480899768288
	1480899768288 [label=AccumulateGrad]
	1480899767664 -> 1480899767040
	1480899837488 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	1480899837488 -> 1480899767664
	1480899767664 [label=AccumulateGrad]
	1480899767568 -> 1480899767040
	1480899837568 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	1480899837568 -> 1480899767568
	1480899767568 [label=AccumulateGrad]
	1480899767088 -> 1480899766896
	1481012463648 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1481012463648 -> 1480899767088
	1480899767088 [label=AccumulateGrad]
	1480899766944 -> 1480899766800
	1481012463728 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	1481012463728 -> 1480899766944
	1480899766944 [label=AccumulateGrad]
	1480899766752 -> 1480899766800
	1481012463808 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	1481012463808 -> 1480899766752
	1480899766752 [label=AccumulateGrad]
	1480899766704 -> 1480899766464
	1481012464288 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1481012464288 -> 1480899766704
	1480899766704 [label=AccumulateGrad]
	1480899766512 -> 1480899766368
	1481012464368 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	1481012464368 -> 1480899766512
	1480899766512 [label=AccumulateGrad]
	1480899766320 -> 1480899766368
	1481012464448 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	1481012464448 -> 1480899766320
	1480899766320 [label=AccumulateGrad]
	1480899766272 -> 1480899766032
	1481012465408 [label="layer4.1.conv3.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	1481012465408 -> 1480899766272
	1480899766272 [label=AccumulateGrad]
	1480899766080 -> 1480899765888
	1481012464848 [label="layer4.1.bn3.weight
 (512)" fillcolor=lightblue]
	1481012464848 -> 1480899766080
	1480899766080 [label=AccumulateGrad]
	1480899765984 -> 1480899765888
	1481012464928 [label="layer4.1.bn3.bias
 (512)" fillcolor=lightblue]
	1481012464928 -> 1480899765984
	1480899765984 [label=AccumulateGrad]
	1480899765936 -> 1480899765792
	1480899765696 -> 1480899765456
	1481012466208 [label="layer5.0.conv1.weight
 (1024, 512, 3, 3)" fillcolor=lightblue]
	1481012466208 -> 1480899765696
	1480899765696 [label=AccumulateGrad]
	1480899765504 -> 1480899765360
	1481012466288 [label="layer5.0.bn1.weight
 (1024)" fillcolor=lightblue]
	1481012466288 -> 1480899765504
	1480899765504 [label=AccumulateGrad]
	1480899765312 -> 1480899765360
	1481012466368 [label="layer5.0.bn1.bias
 (1024)" fillcolor=lightblue]
	1481012466368 -> 1480899765312
	1480899765312 [label=AccumulateGrad]
	1480899765264 -> 1480899765024
	1481012466848 [label="layer5.0.conv2.weight
 (1024, 1024, 3, 3)" fillcolor=lightblue]
	1481012466848 -> 1480899765264
	1480899765264 [label=AccumulateGrad]
	1480899765072 -> 1480899764928
	1481012466928 [label="layer5.0.bn2.weight
 (1024)" fillcolor=lightblue]
	1481012466928 -> 1480899765072
	1480899765072 [label=AccumulateGrad]
	1480899764880 -> 1480899764928
	1481012467008 [label="layer5.0.bn2.bias
 (1024)" fillcolor=lightblue]
	1481012467008 -> 1480899764880
	1480899764880 [label=AccumulateGrad]
	1480899764832 -> 1480899764592
	1481012467968 [label="layer5.0.conv3.weight
 (1024, 1024, 3, 3)" fillcolor=lightblue]
	1481012467968 -> 1480899764832
	1480899764832 [label=AccumulateGrad]
	1480899764640 -> 1480899764448
	1481012467408 [label="layer5.0.bn3.weight
 (1024)" fillcolor=lightblue]
	1481012467408 -> 1480899764640
	1480899764640 [label=AccumulateGrad]
	1480899764544 -> 1480899764448
	1481012467488 [label="layer5.0.bn3.bias
 (1024)" fillcolor=lightblue]
	1481012467488 -> 1480899764544
	1480899764544 [label=AccumulateGrad]
	1480899764496 -> 1480899764352
	1480899764496 [label=CudnnBatchNormBackward0]
	1480899764976 -> 1480899764496
	1480899764976 [label=ConvolutionBackward0]
	1480899765648 -> 1480899764976
	1480899765168 -> 1480899764976
	1481012465568 [label="layer5.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	1481012465568 -> 1480899765168
	1480899765168 [label=AccumulateGrad]
	1480899764736 -> 1480899764496
	1481012465648 [label="layer5.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	1481012465648 -> 1480899764736
	1480899764736 [label=AccumulateGrad]
	1480899764688 -> 1480899764496
	1481012465728 [label="layer5.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	1481012465728 -> 1480899764688
	1480899764688 [label=AccumulateGrad]
	1480899764256 -> 1480899764064
	1481012468128 [label="layer5.1.conv1.weight
 (1024, 1024, 3, 3)" fillcolor=lightblue]
	1481012468128 -> 1480899764256
	1480899764256 [label=AccumulateGrad]
	1480899764112 -> 1480899763632
	1481012468208 [label="layer5.1.bn1.weight
 (1024)" fillcolor=lightblue]
	1481012468208 -> 1480899764112
	1480899764112 [label=AccumulateGrad]
	1480899763776 -> 1480899763632
	1481012468288 [label="layer5.1.bn1.bias
 (1024)" fillcolor=lightblue]
	1481012468288 -> 1480899763776
	1480899763776 [label=AccumulateGrad]
	1480899763056 -> 1480899768192
	1481012468768 [label="layer5.1.conv2.weight
 (1024, 1024, 3, 3)" fillcolor=lightblue]
	1481012468768 -> 1480899763056
	1480899763056 [label=AccumulateGrad]
	1480899768576 -> 1480899769296
	1481012468848 [label="layer5.1.bn2.weight
 (1024)" fillcolor=lightblue]
	1481012468848 -> 1480899768576
	1480899768576 [label=AccumulateGrad]
	1480899769248 -> 1480899769296
	1481012468928 [label="layer5.1.bn2.bias
 (1024)" fillcolor=lightblue]
	1481012468928 -> 1480899769248
	1480899769248 [label=AccumulateGrad]
	1480899769200 -> 1480897587776
	1481012469888 [label="layer5.1.conv3.weight
 (1024, 1024, 3, 3)" fillcolor=lightblue]
	1481012469888 -> 1480899769200
	1480899769200 [label=AccumulateGrad]
	1480897588400 -> 1480897587680
	1481012469328 [label="layer5.1.bn3.weight
 (1024)" fillcolor=lightblue]
	1481012469328 -> 1480897588400
	1480897588400 [label=AccumulateGrad]
	1480899763584 -> 1480897587680
	1481012469408 [label="layer5.1.bn3.bias
 (1024)" fillcolor=lightblue]
	1481012469408 -> 1480899763584
	1480899763584 [label=AccumulateGrad]
	1480897589696 -> 1480897588496
	1480895618480 -> 1480899439264
	1480895618480 [label=TBackward0]
	1480890581344 -> 1480895618480
	1481012469968 [label="classifier.2.weight
 (2048, 1024)" fillcolor=lightblue]
	1481012469968 -> 1480890581344
	1480890581344 [label=AccumulateGrad]
	1480897845264 -> 1480897847856
	1480897845264 [label=TBackward0]
	1480895947840 -> 1480897845264
	1481012469808 [label="classifier.5.weight
 (6, 2048)" fillcolor=lightblue]
	1481012469808 -> 1480895947840
	1480895947840 [label=AccumulateGrad]
	1480897847856 -> 1481012476288
}
