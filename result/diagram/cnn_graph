digraph {
	graph [size="103.64999999999999,103.64999999999999"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2294346609952 [label="
 (2, 10)" fillcolor=darkolivegreen1]
	2294222827968 [label=AddmmBackward0]
	2294224702384 -> 2294222827968
	2294346604032 [label="classifier.5.bias
 (10)" fillcolor=lightblue]
	2294346604032 -> 2294224702384
	2294224702384 [label=AccumulateGrad]
	2294222828112 -> 2294222827968
	2294222828112 [label=NativeDropoutBackward0]
	2294221658896 -> 2294222828112
	2294221658896 [label=ReluBackward0]
	2294222624304 -> 2294221658896
	2294222624304 [label=AddmmBackward0]
	2294222624352 -> 2294222624304
	2294226687776 [label="classifier.2.bias
 (2048)" fillcolor=lightblue]
	2294226687776 -> 2294222624352
	2294222624352 [label=AccumulateGrad]
	2294222626896 -> 2294222624304
	2294222626896 [label=NativeDropoutBackward0]
	2294226299552 -> 2294222626896
	2294226299552 [label=ViewBackward0]
	2294226297680 -> 2294226299552
	2294226297680 [label=MeanBackward1]
	2294226298736 -> 2294226297680
	2294226298736 [label=NativeDropoutBackward0]
	2294224447248 -> 2294226298736
	2294224447248 [label=ReluBackward0]
	2294224442160 -> 2294224447248
	2294224442160 [label=AddBackward0]
	2294224444224 -> 2294224442160
	2294224444224 [label=CudnnBatchNormBackward0]
	2294224448112 -> 2294224444224
	2294224448112 [label=ConvolutionBackward0]
	2294226623872 -> 2294224448112
	2294226623872 [label=ReluBackward0]
	2294226618304 -> 2294226623872
	2294226618304 [label=CudnnBatchNormBackward0]
	2294226623248 -> 2294226618304
	2294226623248 [label=ConvolutionBackward0]
	2294226624496 -> 2294226623248
	2294226624496 [label=ReluBackward0]
	2294226624640 -> 2294226624496
	2294226624640 [label=CudnnBatchNormBackward0]
	2294226624736 -> 2294226624640
	2294226624736 [label=ConvolutionBackward0]
	2294224452144 -> 2294226624736
	2294224452144 [label=ReluBackward0]
	2294226625024 -> 2294224452144
	2294226625024 [label=AddBackward0]
	2294226625120 -> 2294226625024
	2294226625120 [label=CudnnBatchNormBackward0]
	2294226625264 -> 2294226625120
	2294226625264 [label=ConvolutionBackward0]
	2294226625456 -> 2294226625264
	2294226625456 [label=ReluBackward0]
	2294226625600 -> 2294226625456
	2294226625600 [label=CudnnBatchNormBackward0]
	2294226625696 -> 2294226625600
	2294226625696 [label=ConvolutionBackward0]
	2294226625888 -> 2294226625696
	2294226625888 [label=ReluBackward0]
	2294226626032 -> 2294226625888
	2294226626032 [label=CudnnBatchNormBackward0]
	2294226626128 -> 2294226626032
	2294226626128 [label=ConvolutionBackward0]
	2294226626320 -> 2294226626128
	2294226626320 [label=ReluBackward0]
	2294226626464 -> 2294226626320
	2294226626464 [label=AddBackward0]
	2294226626560 -> 2294226626464
	2294226626560 [label=CudnnBatchNormBackward0]
	2294226626704 -> 2294226626560
	2294226626704 [label=ConvolutionBackward0]
	2294226626896 -> 2294226626704
	2294226626896 [label=ReluBackward0]
	2294226627040 -> 2294226626896
	2294226627040 [label=CudnnBatchNormBackward0]
	2294226627136 -> 2294226627040
	2294226627136 [label=ConvolutionBackward0]
	2294226627328 -> 2294226627136
	2294226627328 [label=ReluBackward0]
	2294226627472 -> 2294226627328
	2294226627472 [label=CudnnBatchNormBackward0]
	2294226627568 -> 2294226627472
	2294226627568 [label=ConvolutionBackward0]
	2294226626608 -> 2294226627568
	2294226626608 [label=ReluBackward0]
	2294226627856 -> 2294226626608
	2294226627856 [label=AddBackward0]
	2294226628000 -> 2294226627856
	2294226628000 [label=CudnnBatchNormBackward0]
	2294226628144 -> 2294226628000
	2294226628144 [label=ConvolutionBackward0]
	2294226628336 -> 2294226628144
	2294226628336 [label=ReluBackward0]
	2294226628480 -> 2294226628336
	2294226628480 [label=CudnnBatchNormBackward0]
	2294226628624 -> 2294226628480
	2294226628624 [label=ConvolutionBackward0]
	2294226628816 -> 2294226628624
	2294226628816 [label=ReluBackward0]
	2294226628960 -> 2294226628816
	2294226628960 [label=CudnnBatchNormBackward0]
	2294226629104 -> 2294226628960
	2294226629104 [label=ConvolutionBackward0]
	2294226629296 -> 2294226629104
	2294226629296 [label=ReluBackward0]
	2294226629440 -> 2294226629296
	2294226629440 [label=AddBackward0]
	2294226629584 -> 2294226629440
	2294226629584 [label=CudnnBatchNormBackward0]
	2294226629728 -> 2294226629584
	2294226629728 [label=ConvolutionBackward0]
	2294226629920 -> 2294226629728
	2294226629920 [label=ReluBackward0]
	2294226630064 -> 2294226629920
	2294226630064 [label=CudnnBatchNormBackward0]
	2294226630208 -> 2294226630064
	2294226630208 [label=ConvolutionBackward0]
	2294226630400 -> 2294226630208
	2294226630400 [label=ReluBackward0]
	2294226630544 -> 2294226630400
	2294226630544 [label=CudnnBatchNormBackward0]
	2294226630688 -> 2294226630544
	2294226630688 [label=ConvolutionBackward0]
	2294226629392 -> 2294226630688
	2294226629392 [label=ReluBackward0]
	2294226630976 -> 2294226629392
	2294226630976 [label=AddBackward0]
	2294226631120 -> 2294226630976
	2294226631120 [label=CudnnBatchNormBackward0]
	2294226631264 -> 2294226631120
	2294226631264 [label=ConvolutionBackward0]
	2294226631456 -> 2294226631264
	2294226631456 [label=ReluBackward0]
	2294226631600 -> 2294226631456
	2294226631600 [label=CudnnBatchNormBackward0]
	2294226631744 -> 2294226631600
	2294226631744 [label=ConvolutionBackward0]
	2294226631936 -> 2294226631744
	2294226631936 [label=ReluBackward0]
	2294226632080 -> 2294226631936
	2294226632080 [label=CudnnBatchNormBackward0]
	2294226632224 -> 2294226632080
	2294226632224 [label=ConvolutionBackward0]
	2294226632416 -> 2294226632224
	2294226632416 [label=ReluBackward0]
	2294226632560 -> 2294226632416
	2294226632560 [label=AddBackward0]
	2294226632656 -> 2294226632560
	2294226632656 [label=CudnnBatchNormBackward0]
	2294346760400 -> 2294226632656
	2294346760400 [label=ConvolutionBackward0]
	2294346760592 -> 2294346760400
	2294346760592 [label=ReluBackward0]
	2294346760736 -> 2294346760592
	2294346760736 [label=CudnnBatchNormBackward0]
	2294346760880 -> 2294346760736
	2294346760880 [label=ConvolutionBackward0]
	2294346761072 -> 2294346760880
	2294346761072 [label=ReluBackward0]
	2294346761216 -> 2294346761072
	2294346761216 [label=CudnnBatchNormBackward0]
	2294346761360 -> 2294346761216
	2294346761360 [label=ConvolutionBackward0]
	2294346760256 -> 2294346761360
	2294346760256 [label=ReluBackward0]
	2294346761648 -> 2294346760256
	2294346761648 [label=AddBackward0]
	2294346761792 -> 2294346761648
	2294346761792 [label=CudnnBatchNormBackward0]
	2294346761936 -> 2294346761792
	2294346761936 [label=ConvolutionBackward0]
	2294346762128 -> 2294346761936
	2294346762128 [label=ReluBackward0]
	2294346762272 -> 2294346762128
	2294346762272 [label=CudnnBatchNormBackward0]
	2294346762416 -> 2294346762272
	2294346762416 [label=ConvolutionBackward0]
	2294346762608 -> 2294346762416
	2294346762608 [label=ReluBackward0]
	2294346762752 -> 2294346762608
	2294346762752 [label=CudnnBatchNormBackward0]
	2294346762896 -> 2294346762752
	2294346762896 [label=ConvolutionBackward0]
	2294346763088 -> 2294346762896
	2294346763088 [label=ReluBackward0]
	2294346763232 -> 2294346763088
	2294346763232 [label=AddBackward0]
	2294346763376 -> 2294346763232
	2294346763376 [label=CudnnBatchNormBackward0]
	2294346763520 -> 2294346763376
	2294346763520 [label=ConvolutionBackward0]
	2294346763712 -> 2294346763520
	2294346763712 [label=ReluBackward0]
	2294346763856 -> 2294346763712
	2294346763856 [label=CudnnBatchNormBackward0]
	2294346764000 -> 2294346763856
	2294346764000 [label=ConvolutionBackward0]
	2294346764192 -> 2294346764000
	2294346764192 [label=ReluBackward0]
	2294346764336 -> 2294346764192
	2294346764336 [label=CudnnBatchNormBackward0]
	2294346764480 -> 2294346764336
	2294346764480 [label=ConvolutionBackward0]
	2294346763184 -> 2294346764480
	2294346763184 [label=ReluBackward0]
	2294346764768 -> 2294346763184
	2294346764768 [label=AddBackward0]
	2294346764912 -> 2294346764768
	2294346764912 [label=CudnnBatchNormBackward0]
	2294346765056 -> 2294346764912
	2294346765056 [label=ConvolutionBackward0]
	2294346765248 -> 2294346765056
	2294346765248 [label=ReluBackward0]
	2294346765392 -> 2294346765248
	2294346765392 [label=CudnnBatchNormBackward0]
	2294346765536 -> 2294346765392
	2294346765536 [label=ConvolutionBackward0]
	2294346765728 -> 2294346765536
	2294346765728 [label=ReluBackward0]
	2294346765872 -> 2294346765728
	2294346765872 [label=CudnnBatchNormBackward0]
	2294346766016 -> 2294346765872
	2294346766016 [label=ConvolutionBackward0]
	2294346764624 -> 2294346766016
	2294346764624 [label=MaxPool2DWithIndicesBackward0]
	2294346766304 -> 2294346764624
	2294346766304 [label=ReluBackward0]
	2294346766448 -> 2294346766304
	2294346766448 [label=CudnnBatchNormBackward0]
	2294346766592 -> 2294346766448
	2294346766592 [label=ConvolutionBackward0]
	2294346766784 -> 2294346766592
	2293635049792 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	2293635049792 -> 2294346766784
	2294346766784 [label=AccumulateGrad]
	2294346766496 -> 2294346766448
	2293635049712 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2293635049712 -> 2294346766496
	2294346766496 [label=AccumulateGrad]
	2294346766640 -> 2294346766448
	2293635042992 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2293635042992 -> 2294346766640
	2294346766640 [label=AccumulateGrad]
	2294346766208 -> 2294346766016
	2293635042752 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2293635042752 -> 2294346766208
	2294346766208 [label=AccumulateGrad]
	2294346765824 -> 2294346765872
	2293635049312 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2293635049312 -> 2294346765824
	2294346765824 [label=AccumulateGrad]
	2294346766064 -> 2294346765872
	2293635049232 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2293635049232 -> 2294346766064
	2294346766064 [label=AccumulateGrad]
	2294346765776 -> 2294346765536
	2293635042272 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2293635042272 -> 2294346765776
	2294346765776 [label=AccumulateGrad]
	2294346765344 -> 2294346765392
	2293635048992 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2293635048992 -> 2294346765344
	2294346765344 [label=AccumulateGrad]
	2294346765584 -> 2294346765392
	2293635048912 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2293635048912 -> 2294346765584
	2294346765584 [label=AccumulateGrad]
	2294346765296 -> 2294346765056
	2293635048512 [label="layer1.0.conv3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2293635048512 -> 2294346765296
	2294346765296 [label=AccumulateGrad]
	2294346765104 -> 2294346764912
	2293635042032 [label="layer1.0.bn3.weight
 (64)" fillcolor=lightblue]
	2293635042032 -> 2294346765104
	2294346765104 [label=AccumulateGrad]
	2294346765008 -> 2294346764912
	2293635041952 [label="layer1.0.bn3.bias
 (64)" fillcolor=lightblue]
	2293635041952 -> 2294346765008
	2294346765008 [label=AccumulateGrad]
	2294346764624 -> 2294346764768
	2294346764672 -> 2294346764480
	2293635041712 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2293635041712 -> 2294346764672
	2294346764672 [label=AccumulateGrad]
	2294346764288 -> 2294346764336
	2293635049472 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2293635049472 -> 2294346764288
	2294346764288 [label=AccumulateGrad]
	2294346764528 -> 2294346764336
	2293635049392 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2293635049392 -> 2294346764528
	2294346764528 [label=AccumulateGrad]
	2294346764240 -> 2294346764000
	2293635040752 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2293635040752 -> 2294346764240
	2294346764240 [label=AccumulateGrad]
	2294346763808 -> 2294346763856
	2293635047152 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2293635047152 -> 2294346763808
	2294346763808 [label=AccumulateGrad]
	2294346764048 -> 2294346763856
	2293635047072 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2293635047072 -> 2294346764048
	2294346764048 [label=AccumulateGrad]
	2294346763760 -> 2294346763520
	2294226688096 [label="layer1.1.conv3.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2294226688096 -> 2294346763760
	2294346763760 [label=AccumulateGrad]
	2294346763568 -> 2294346763376
	2294226687536 [label="layer1.1.bn3.weight
 (64)" fillcolor=lightblue]
	2294226687536 -> 2294346763568
	2294346763568 [label=AccumulateGrad]
	2294346763472 -> 2294346763376
	2294226687616 [label="layer1.1.bn3.bias
 (64)" fillcolor=lightblue]
	2294226687616 -> 2294346763472
	2294346763472 [label=AccumulateGrad]
	2294346763184 -> 2294346763232
	2294346763136 -> 2294346762896
	2294226688896 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2294226688896 -> 2294346763136
	2294346763136 [label=AccumulateGrad]
	2294346762704 -> 2294346762752
	2294226688976 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2294226688976 -> 2294346762704
	2294346762704 [label=AccumulateGrad]
	2294346762944 -> 2294346762752
	2294226689056 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2294226689056 -> 2294346762944
	2294346762944 [label=AccumulateGrad]
	2294346762656 -> 2294346762416
	2294226689536 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2294226689536 -> 2294346762656
	2294346762656 [label=AccumulateGrad]
	2294346762224 -> 2294346762272
	2294226689616 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2294226689616 -> 2294346762224
	2294346762224 [label=AccumulateGrad]
	2294346762464 -> 2294346762272
	2294226689696 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2294226689696 -> 2294346762464
	2294346762464 [label=AccumulateGrad]
	2294346762176 -> 2294346761936
	2294226690656 [label="layer2.0.conv3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2294226690656 -> 2294346762176
	2294346762176 [label=AccumulateGrad]
	2294346761984 -> 2294346761792
	2294226690096 [label="layer2.0.bn3.weight
 (128)" fillcolor=lightblue]
	2294226690096 -> 2294346761984
	2294346761984 [label=AccumulateGrad]
	2294346761888 -> 2294346761792
	2294226690176 [label="layer2.0.bn3.bias
 (128)" fillcolor=lightblue]
	2294226690176 -> 2294346761888
	2294346761888 [label=AccumulateGrad]
	2294346761504 -> 2294346761648
	2294346761504 [label=CudnnBatchNormBackward0]
	2293721001568 -> 2294346761504
	2293721001568 [label=ConvolutionBackward0]
	2294346763088 -> 2293721001568
	2294346762560 -> 2293721001568
	2294226688256 [label="layer2.0.downsample.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	2294226688256 -> 2294346762560
	2294346762560 [label=AccumulateGrad]
	2294346762080 -> 2294346761504
	2294226688336 [label="layer2.0.downsample.1.weight
 (128)" fillcolor=lightblue]
	2294226688336 -> 2294346762080
	2294346762080 [label=AccumulateGrad]
	2294346762032 -> 2294346761504
	2294226688416 [label="layer2.0.downsample.1.bias
 (128)" fillcolor=lightblue]
	2294226688416 -> 2294346762032
	2294346762032 [label=AccumulateGrad]
	2294346761552 -> 2294346761360
	2294226690816 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2294226690816 -> 2294346761552
	2294346761552 [label=AccumulateGrad]
	2294346761168 -> 2294346761216
	2294226690896 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2294226690896 -> 2294346761168
	2294346761168 [label=AccumulateGrad]
	2294346761408 -> 2294346761216
	2294226690976 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2294226690976 -> 2294346761408
	2294346761408 [label=AccumulateGrad]
	2294346761120 -> 2294346760880
	2294226691456 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2294226691456 -> 2294346761120
	2294346761120 [label=AccumulateGrad]
	2294346760688 -> 2294346760736
	2294226691536 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2294226691536 -> 2294346760688
	2294346760688 [label=AccumulateGrad]
	2294346760928 -> 2294346760736
	2294226691616 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2294226691616 -> 2294346760928
	2294346760928 [label=AccumulateGrad]
	2294346760640 -> 2294346760400
	2294226692576 [label="layer2.1.conv3.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2294226692576 -> 2294346760640
	2294346760640 [label=AccumulateGrad]
	2294346760448 -> 2294226632656
	2294226692016 [label="layer2.1.bn3.weight
 (128)" fillcolor=lightblue]
	2294226692016 -> 2294346760448
	2294346760448 [label=AccumulateGrad]
	2294346760352 -> 2294226632656
	2294226692096 [label="layer2.1.bn3.bias
 (128)" fillcolor=lightblue]
	2294226692096 -> 2294346760352
	2294346760352 [label=AccumulateGrad]
	2294346760256 -> 2294226632560
	2294226632464 -> 2294226632224
	2294226693296 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	2294226693296 -> 2294226632464
	2294226632464 [label=AccumulateGrad]
	2294226632032 -> 2294226632080
	2294226693376 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2294226693376 -> 2294226632032
	2294226632032 [label=AccumulateGrad]
	2294226632272 -> 2294226632080
	2294226693456 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2294226693456 -> 2294226632272
	2294226632272 [label=AccumulateGrad]
	2294226631984 -> 2294226631744
	2294226693936 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2294226693936 -> 2294226631984
	2294226631984 [label=AccumulateGrad]
	2294226631552 -> 2294226631600
	2294226694016 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2294226694016 -> 2294226631552
	2294226631552 [label=AccumulateGrad]
	2294226631792 -> 2294226631600
	2294226694096 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2294226694096 -> 2294226631792
	2294226631792 [label=AccumulateGrad]
	2294226631504 -> 2294226631264
	2294226695056 [label="layer3.0.conv3.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2294226695056 -> 2294226631504
	2294226631504 [label=AccumulateGrad]
	2294226631312 -> 2294226631120
	2294226694496 [label="layer3.0.bn3.weight
 (256)" fillcolor=lightblue]
	2294226694496 -> 2294226631312
	2294226631312 [label=AccumulateGrad]
	2294226631216 -> 2294226631120
	2294226694576 [label="layer3.0.bn3.bias
 (256)" fillcolor=lightblue]
	2294226694576 -> 2294226631216
	2294226631216 [label=AccumulateGrad]
	2294226630832 -> 2294226630976
	2294226630832 [label=CudnnBatchNormBackward0]
	2294226631648 -> 2294226630832
	2294226631648 [label=ConvolutionBackward0]
	2294226632416 -> 2294226631648
	2294226631888 -> 2294226631648
	2294226692656 [label="layer3.0.downsample.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	2294226692656 -> 2294226631888
	2294226631888 [label=AccumulateGrad]
	2294226631408 -> 2294226630832
	2294226692736 [label="layer3.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2294226692736 -> 2294226631408
	2294226631408 [label=AccumulateGrad]
	2294226631360 -> 2294226630832
	2294226692816 [label="layer3.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2294226692816 -> 2294226631360
	2294226631360 [label=AccumulateGrad]
	2294226630880 -> 2294226630688
	2294226695216 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2294226695216 -> 2294226630880
	2294226630880 [label=AccumulateGrad]
	2294226630496 -> 2294226630544
	2294226695296 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2294226695296 -> 2294226630496
	2294226630496 [label=AccumulateGrad]
	2294226630736 -> 2294226630544
	2294226695376 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2294226695376 -> 2294226630736
	2294226630736 [label=AccumulateGrad]
	2294226630448 -> 2294226630208
	2294226695856 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2294226695856 -> 2294226630448
	2294226630448 [label=AccumulateGrad]
	2294226630016 -> 2294226630064
	2294226695936 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2294226695936 -> 2294226630016
	2294226630016 [label=AccumulateGrad]
	2294226630256 -> 2294226630064
	2294226696016 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2294226696016 -> 2294226630256
	2294226630256 [label=AccumulateGrad]
	2294226629968 -> 2294226629728
	2294226696976 [label="layer3.1.conv3.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2294226696976 -> 2294226629968
	2294226629968 [label=AccumulateGrad]
	2294226629776 -> 2294226629584
	2294226696416 [label="layer3.1.bn3.weight
 (256)" fillcolor=lightblue]
	2294226696416 -> 2294226629776
	2294226629776 [label=AccumulateGrad]
	2294226629680 -> 2294226629584
	2294226696496 [label="layer3.1.bn3.bias
 (256)" fillcolor=lightblue]
	2294226696496 -> 2294226629680
	2294226629680 [label=AccumulateGrad]
	2294226629392 -> 2294226629440
	2294226629344 -> 2294226629104
	2294226697776 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	2294226697776 -> 2294226629344
	2294226629344 [label=AccumulateGrad]
	2294226628912 -> 2294226628960
	2294226697856 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2294226697856 -> 2294226628912
	2294226628912 [label=AccumulateGrad]
	2294226629152 -> 2294226628960
	2294226697936 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2294226697936 -> 2294226629152
	2294226629152 [label=AccumulateGrad]
	2294226628864 -> 2294226628624
	2294346596592 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2294346596592 -> 2294226628864
	2294226628864 [label=AccumulateGrad]
	2294226628432 -> 2294226628480
	2294346596672 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2294346596672 -> 2294226628432
	2294226628432 [label=AccumulateGrad]
	2294226628672 -> 2294226628480
	2294346596752 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2294346596752 -> 2294226628672
	2294226628672 [label=AccumulateGrad]
	2294226628384 -> 2294226628144
	2294346597712 [label="layer4.0.conv3.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2294346597712 -> 2294226628384
	2294226628384 [label=AccumulateGrad]
	2294226628192 -> 2294226628000
	2294346597152 [label="layer4.0.bn3.weight
 (512)" fillcolor=lightblue]
	2294346597152 -> 2294226628192
	2294226628192 [label=AccumulateGrad]
	2294226628096 -> 2294226628000
	2294346597232 [label="layer4.0.bn3.bias
 (512)" fillcolor=lightblue]
	2294346597232 -> 2294226628096
	2294226628096 [label=AccumulateGrad]
	2294226627712 -> 2294226627856
	2294226627712 [label=CudnnBatchNormBackward0]
	2294226628528 -> 2294226627712
	2294226628528 [label=ConvolutionBackward0]
	2294226629296 -> 2294226628528
	2294226628768 -> 2294226628528
	2294226697136 [label="layer4.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2294226697136 -> 2294226628768
	2294226628768 [label=AccumulateGrad]
	2294226628288 -> 2294226627712
	2294226697216 [label="layer4.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2294226697216 -> 2294226628288
	2294226628288 [label=AccumulateGrad]
	2294226628240 -> 2294226627712
	2294226697296 [label="layer4.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2294226697296 -> 2294226628240
	2294226628240 [label=AccumulateGrad]
	2294226627760 -> 2294226627568
	2294346597872 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2294346597872 -> 2294226627760
	2294226627760 [label=AccumulateGrad]
	2294226627616 -> 2294226627472
	2294346597952 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2294346597952 -> 2294226627616
	2294226627616 [label=AccumulateGrad]
	2294226627424 -> 2294226627472
	2294346598032 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2294346598032 -> 2294226627424
	2294226627424 [label=AccumulateGrad]
	2294226627376 -> 2294226627136
	2294346598512 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2294346598512 -> 2294226627376
	2294226627376 [label=AccumulateGrad]
	2294226627184 -> 2294226627040
	2294346598592 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2294346598592 -> 2294226627184
	2294226627184 [label=AccumulateGrad]
	2294226626992 -> 2294226627040
	2294346598672 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2294346598672 -> 2294226626992
	2294226626992 [label=AccumulateGrad]
	2294226626944 -> 2294226626704
	2294346599632 [label="layer4.1.conv3.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2294346599632 -> 2294226626944
	2294226626944 [label=AccumulateGrad]
	2294226626752 -> 2294226626560
	2294346599072 [label="layer4.1.bn3.weight
 (512)" fillcolor=lightblue]
	2294346599072 -> 2294226626752
	2294226626752 [label=AccumulateGrad]
	2294226626656 -> 2294226626560
	2294346599152 [label="layer4.1.bn3.bias
 (512)" fillcolor=lightblue]
	2294346599152 -> 2294226626656
	2294226626656 [label=AccumulateGrad]
	2294226626608 -> 2294226626464
	2294226626368 -> 2294226626128
	2294346600432 [label="layer5.0.conv1.weight
 (1024, 512, 3, 3)" fillcolor=lightblue]
	2294346600432 -> 2294226626368
	2294226626368 [label=AccumulateGrad]
	2294226626176 -> 2294226626032
	2294346600512 [label="layer5.0.bn1.weight
 (1024)" fillcolor=lightblue]
	2294346600512 -> 2294226626176
	2294226626176 [label=AccumulateGrad]
	2294226625984 -> 2294226626032
	2294346600592 [label="layer5.0.bn1.bias
 (1024)" fillcolor=lightblue]
	2294346600592 -> 2294226625984
	2294226625984 [label=AccumulateGrad]
	2294226625936 -> 2294226625696
	2294346601072 [label="layer5.0.conv2.weight
 (1024, 1024, 3, 3)" fillcolor=lightblue]
	2294346601072 -> 2294226625936
	2294226625936 [label=AccumulateGrad]
	2294226625744 -> 2294226625600
	2294346601152 [label="layer5.0.bn2.weight
 (1024)" fillcolor=lightblue]
	2294346601152 -> 2294226625744
	2294226625744 [label=AccumulateGrad]
	2294226625552 -> 2294226625600
	2294346601232 [label="layer5.0.bn2.bias
 (1024)" fillcolor=lightblue]
	2294346601232 -> 2294226625552
	2294226625552 [label=AccumulateGrad]
	2294226625504 -> 2294226625264
	2294346602192 [label="layer5.0.conv3.weight
 (1024, 1024, 3, 3)" fillcolor=lightblue]
	2294346602192 -> 2294226625504
	2294226625504 [label=AccumulateGrad]
	2294226625312 -> 2294226625120
	2294346601632 [label="layer5.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2294346601632 -> 2294226625312
	2294226625312 [label=AccumulateGrad]
	2294226625216 -> 2294226625120
	2294346601712 [label="layer5.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2294346601712 -> 2294226625216
	2294226625216 [label=AccumulateGrad]
	2294226625168 -> 2294226625024
	2294226625168 [label=CudnnBatchNormBackward0]
	2294226625648 -> 2294226625168
	2294226625648 [label=ConvolutionBackward0]
	2294226626320 -> 2294226625648
	2294226625840 -> 2294226625648
	2294346599792 [label="layer5.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2294346599792 -> 2294226625840
	2294226625840 [label=AccumulateGrad]
	2294226625408 -> 2294226625168
	2294346599872 [label="layer5.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2294346599872 -> 2294226625408
	2294226625408 [label=AccumulateGrad]
	2294226625360 -> 2294226625168
	2294346599952 [label="layer5.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2294346599952 -> 2294226625360
	2294226625360 [label=AccumulateGrad]
	2294226624928 -> 2294226624736
	2294346602352 [label="layer5.1.conv1.weight
 (1024, 1024, 3, 3)" fillcolor=lightblue]
	2294346602352 -> 2294226624928
	2294226624928 [label=AccumulateGrad]
	2294226624784 -> 2294226624640
	2294346602432 [label="layer5.1.bn1.weight
 (1024)" fillcolor=lightblue]
	2294346602432 -> 2294226624784
	2294226624784 [label=AccumulateGrad]
	2294226624592 -> 2294226624640
	2294346602512 [label="layer5.1.bn1.bias
 (1024)" fillcolor=lightblue]
	2294346602512 -> 2294226624592
	2294226624592 [label=AccumulateGrad]
	2294226624544 -> 2294226623248
	2294346602992 [label="layer5.1.conv2.weight
 (1024, 1024, 3, 3)" fillcolor=lightblue]
	2294346602992 -> 2294226624544
	2294226624544 [label=AccumulateGrad]
	2294226624256 -> 2294226618304
	2294346603072 [label="layer5.1.bn2.weight
 (1024)" fillcolor=lightblue]
	2294346603072 -> 2294226624256
	2294226624256 [label=AccumulateGrad]
	2294226623776 -> 2294226618304
	2294346603152 [label="layer5.1.bn2.bias
 (1024)" fillcolor=lightblue]
	2294346603152 -> 2294226623776
	2294226623776 [label=AccumulateGrad]
	2294226623824 -> 2294224448112
	2294346604112 [label="layer5.1.conv3.weight
 (1024, 1024, 3, 3)" fillcolor=lightblue]
	2294346604112 -> 2294226623824
	2294226623824 [label=AccumulateGrad]
	2294224453200 -> 2294224444224
	2294346603552 [label="layer5.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2294346603552 -> 2294224453200
	2294224453200 [label=AccumulateGrad]
	2294224453392 -> 2294224444224
	2294346603632 [label="layer5.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2294346603632 -> 2294224453392
	2294224453392 [label=AccumulateGrad]
	2294224452144 -> 2294224442160
	2294222627184 -> 2294222624304
	2294222627184 [label=TBackward0]
	2294226299072 -> 2294222627184
	2294222952384 [label="classifier.2.weight
 (2048, 1024)" fillcolor=lightblue]
	2294222952384 -> 2294226299072
	2294226299072 [label=AccumulateGrad]
	2294222823504 -> 2294222827968
	2294222823504 [label=TBackward0]
	2294221661344 -> 2294222823504
	2294346602672 [label="classifier.5.weight
 (10, 2048)" fillcolor=lightblue]
	2294346602672 -> 2294221661344
	2294221661344 [label=AccumulateGrad]
	2294222827968 -> 2294346609952
}
